// Get data from CSVs
# Import pandas as pd
import pandas as pd

# Read the CSV and assign it to the variable data
data = pd.read_csv("vt_tax_data_2016.csv")

# View the first few lines of data
print(data.head())

// Get data from other flat files
# Import pandas with the alias pd
import pandas as pd

# Load TSV using the sep keyword argument to set delimiter
data = pd.read_csv("vt_tax_data_2016.tsv", sep="\t")

# Plot the total number of tax returns by income group
counts = data.groupby("agi_stub").N1.sum()
counts.plot.bar()
plt.show()

// Import a subset of columns
# Create list of columns to use
cols = ["zipcode", "agi_stub", "mars1", "MARS2", "NUMDEP"]

# Create dataframe from csv using only selected columns
data = pd.read_csv("vt_tax_data_2016.csv", usecols=cols)

# View counts of dependents and tax returns by income level
print(data.groupby("agi_stub").sum())

// Import a file in chunks
# Create dataframe of next 500 rows with labeled columns
vt_data_next500 = pd.read_csv("vt_tax_data_2016.csv", 
                       		  nrows=500,
                       		  skiprows=500,
                       		  header=None,
                       		  names=list(vt_data_first500))

# View the Vermont dataframes to confirm they're different
print(vt_data_first500.head())
print(vt_data_next500.head())
Note: The first 500 rows have been loaded as vt_data_first500. You'll get the next 500 rows. To do this, you'll employ several keyword arguments: nrows and skiprows to get the correct records, header to tell pandas the data does not have column names, and names to supply the missing column names. You'll also want to use the list() function to get column names from vt_data_first500 to reuse.

// Specify data types
# Load csv with no additional arguments
data = pd.read_csv("vt_tax_data_2016.csv")

# Print the data types
print(data.dtypes)

# Create dict specifying that 0s in zipcode are NA values
null_values = {"zipcode": 0}

# Load csv using na_values keyword argument
data = pd.read_csv("vt_tax_data_2016.csv", 
                   na_values=null_values)

# View rows with NA ZIP codes
print(data[data.zipcode.isna()])

// Skip bad data
try:
  # (1) Import the CSV without any keyword arguments
  data = pd.read_csv("vt_tax_data_2016_corrupt.csv")

  # (2) Import CSV with error_bad_lines set to skip bad records
  data = pd.read_csv("vt_tax_data_2016_corrupt.csv", 
                     error_bad_lines=False)

  # (3) Set warn_bad_lines to issue warnings about bad records
  data = pd.read_csv("vt_tax_data_2016_corrupt.csv", 
                     error_bad_lines=False, 
                     warn_bad_lines=True)
  
  # View first 5 records
  print(data.head())
  
except pd.errors.ParserError:
    print("Your data contained rows that could not be parsed.")
=> Output:
(1): <script.py> output:
    Your data contained rows that could not be parsed.
(2): <script.py> output:
       STATEFIPS STATE  zipcode  agi_stub      N1  ...  A85300  N11901  A11901  N11902  A11902
    0         50    VT        0         1  111580  ...       0   10820    9734   88260  138337
    1         50    VT        0         2   82760  ...       0   12820   20029   68760  151729
    2         50    VT        0         3   46270  ...       0   10810   24499   34600   90583
    3         50    VT        0         5   39530  ...       0   12500   67761   23320  103034
    4         50    VT        0         6    9620  ...   20428    3900   93123    2870   39425
    
    [5 rows x 147 columns]
(3): <script.py> output:
    Skipping line 5: expected 147 fields, saw 148
    Skipping line 9: expected 147 fields, saw 148
    Skipping line 51: expected 147 fields, saw 148
    
       STATEFIPS STATE  zipcode  agi_stub      N1  ...  A85300  N11901  A11901  N11902  A11902
    0         50    VT        0         1  111580  ...       0   10820    9734   88260  138337
    1         50    VT        0         2   82760  ...       0   12820   20029   68760  151729
    2         50    VT        0         3   46270  ...       0   10810   24499   34600   90583
    3         50    VT        0         5   39530  ...       0   12500   67761   23320  103034
    4         50    VT        0         6    9620  ...   20428    3900   93123    2870   39425
    
    [5 rows x 147 columns]

// Get data from a spreadsheet
# Load pandas as pd
import pandas as pd

# Read spreadsheet and assign it to survey_responses
survey_responses = pd.read_excel('fcc_survey.xlsx')

# View the head of the dataframe
print(survey_responses.head())

// Load a portion of a spreadsheet
# Create string of lettered columns to load
col_string = "AD, AW:BA"

# Load data with skiprows and usecols set
survey_responses = pd.read_excel("fcc_survey_headers.xlsx", 
                                 skiprows=2, 
                                 usecols=col_string)

# View the names of the columns selected
print(survey_responses.columns)

// Select a single sheet
# Create df from second worksheet by referencing its position
responses_2017 = pd.read_excel("fcc_survey.xlsx",
                               sheet_name=1)

# Create df from second worksheet by referencing its name
responses_2017 = pd.read_excel("fcc_survey.xlsx",
                               sheet_name="2017")

# Graph where people would like to get a developer job
job_prefs = responses_2017.groupby("JobPref").JobPref.count()
job_prefs.plot.barh()
plt.show()

// Select multiple sheets
# Load both the 2016 and 2017 sheets by name
all_survey_data = pd.read_excel("fcc_survey.xlsx",
                                sheet_name=['2016', '2017'])

# View the data type of all_survey_data
print(type(all_survey_data))

# Load all sheets in the Excel file
all_survey_data = pd.read_excel("fcc_survey.xlsx",
                                sheet_name=[0, '2017'])

# View the sheet names in all_survey_data
print(all_survey_data.keys())

# Load all sheets in the Excel file
all_survey_data = pd.read_excel("fcc_survey.xlsx",
                                sheet_name=None)

# View the sheet names in all_survey_data
print(all_survey_data.keys())

// Work with multiple spreadsheets
# Create an empty dataframe
all_responses = pd.DataFrame()

# Set up for loop to iterate through values in responses
for df in responses.values():
  # Print the number of rows being added
  print("Adding {} rows".format(df.shape[0]))
  # Concatenate all_responses and df, assign result
  all_responses = pd.concat([all_responses, df])

# Graph employment statuses in sample
counts = all_responses.groupby("EmploymentStatus").EmploymentStatus.count()
counts.plot.barh()
plt.show()

// Set Boolean columns
# Load the data
survey_data = pd.read_excel("fcc_survey_subset.xlsx")

# Count NA values in each column
print(survey_data.isna().sum())

# Set dtype to load appropriate column(s) as Boolean data
survey_data = pd.read_excel("fcc_survey_subset.xlsx",
                            dtype={"HasDebt": bool})

# View financial burdens by Boolean group
print(survey_data.groupby("HasDebt").sum())
=> Output: <script.py> output:
    ID.x                        0
    HasDebt                     0
    HasFinancialDependents      7
    HasHomeMortgage           499
    HasStudentDebt            502
    dtype: int64

<script.py> output:
             HasFinancialDependents  HasHomeMortgage  HasStudentDebt
    HasDebt                                                         
    False                     112.0              0.0             0.0
    True                      205.0            151.0           281.0

// Set custom true/false values
# Load file with Yes as a True value and No as a False value
survey_subset = pd.read_excel("fcc_survey_yn_data.xlsx",
                              dtype={"HasDebt": bool,
                              "AttendedBootCampYesNo": bool},
                              true_values=["Yes"],
                              false_values=["No"])

# View the data
print(survey_subset.head())
=> Output: <script.py> output:
                                   ID.x  AttendedBootCampYesNo  HasDebt  HasFinancialDependents  HasHomeMortgage  HasStudentDebt
    0  cef35615d61b202f1dc794ef2746df14                  False     True                     1.0              0.0             1.0
    1  323e5a113644d18185c743c241407754                  False    False                     0.0              NaN             NaN
    2  b29a1027e5cd062e654a63764157461d                  False    False                     0.0              NaN             NaN
    3  04a11e4bcb573a1261eb0d9948d32637                  False     True                     0.0              0.0             1.0
    4  9368291c93d5d5f5c8cdb1a575e18bec                  False     True                     0.0              0.0             0.0

// Parse simple dates
# Load file, with Part1StartTime parsed as datetime data
survey_data = pd.read_excel("fcc_survey.xlsx",
                            parse_dates=["Part1StartTime"])

# Print first few values of Part1StartTime
print(survey_data.Part1StartTime.head())
=> Output: <script.py> output:
    0   2016-03-29 21:23:13
    1   2016-03-29 21:24:59
    2   2016-03-29 21:25:37
    3   2016-03-29 21:21:37
    4   2016-03-29 21:26:22
    Name: Part1StartTime, dtype: datetime64[ns]

// Get datetimes from multiple columns
# Create dict of columns to combine into new datetime column
datetime_cols = {"Part2Start": ["Part2StartDate", 
                                "Part2StartTime"]}

# Load file, supplying the dict to parse_dates
survey_data = pd.read_excel("fcc_survey_dts.xlsx",
                            parse_dates=datetime_cols)

# View summary statistics about Part2Start
print(survey_data.Part2Start.describe())
=> Output: <script.py> output:
    count                    1000
    unique                    985
    top       2016-03-30 07:27:25
    freq                        2
    first     2016-03-29 21:24:57
    last      2016-03-30 09:08:18
    Name: Part2Start, dtype: object

// Parse non-standard date formats
# Parse datetimes and assign result back to Part2EndTime
survey_data["Part2EndTime"] = pd.to_datetime(survey_data["Part2EndTime"], 
                                             format="%m%d%Y %H:%M:%S")

# Print first few values of Part2EndTime
print(survey_data.Part2EndTime.head())
=> Output: <script.py> output:
    0   2016-03-29 21:27:25
    1   2016-03-29 21:29:10
    2   2016-03-29 21:28:21
    3   2016-03-29 21:30:51
    4   2016-03-29 21:31:54
    Name: Part2EndTime, dtype: datetime64[ns]

// Connect to a database
# Import sqlalchemy's create_engine() function
from sqlalchemy import create_engine

# Create the database engine
engine = create_engine("sqlite:///data.db")

# View the tables in the database
print(engine.table_names())
=> Output: <script.py> output:
    ['boro_census', 'hpd311calls', 'weather']

// Load entire tables
# Load libraries
import pandas as pd
from sqlalchemy import create_engine

# Create the database engine
engine = create_engine('sqlite:///data.db')

# Load hpd311calls without any SQL
hpd_calls = pd.read_sql("hpd311calls", engine)

# View the first few rows of data
print(hpd_calls.head())

# Create the database engine
engine = create_engine("sqlite:///data.db")

# Create a SQL query to load the entire weather table
query = """
SELECT * 
  FROM weather;
"""

# Load weather with the SQL query
weather = pd.read_sql(query, engine)

# View the first few rows of data
print(weather.head())
=> Output: <script.py> output:
           station                         name  latitude  longitude  elevation  ...  prcp snow  tavg  tmax  tmin
    0  USW00094728  NY CITY CENTRAL PARK, NY US    40.779    -73.969       42.7  ...  0.00  0.0          52    42
    1  USW00094728  NY CITY CENTRAL PARK, NY US    40.779    -73.969       42.7  ...  0.00  0.0          48    39
    2  USW00094728  NY CITY CENTRAL PARK, NY US    40.779    -73.969       42.7  ...  0.00  0.0          48    42
    3  USW00094728  NY CITY CENTRAL PARK, NY US    40.779    -73.969       42.7  ...  0.00  0.0          51    40
    4  USW00094728  NY CITY CENTRAL PARK, NY US    40.779    -73.969       42.7  ...  0.75  0.0          61    50
    
    [5 rows x 13 columns]

// Selecting columns with SQL
# Create database engine for data.db
engine = create_engine('sqlite:///data.db')

# Write query to get date, tmax, and tmin from weather
query = """
SELECT date, 
       tmax, 
       tmin
  FROM weather;
"""

# Make a dataframe by passing query and engine to read_sql()
temperatures = pd.read_sql(query, engine)

# View the resulting dataframe
print(temperatures)

// Selecting rows
# Create query to get hpd311calls records about safety
query = """SELECT *
		     FROM hpd311calls
            WHERE complaint_type = 'SAFETY';"""

# Query the database and assign result to safety_calls
safety_calls = pd.read_sql(query, engine)

# Graph the number of safety calls by borough
call_counts = safety_calls.groupby('borough').unique_key.count()
call_counts.plot.barh()
plt.show()

// Filtering on multiple conditions
# Create query for records with max temps <= 32 or snow >= 1
query = """
SELECT * 
  FROM weather
 WHERE tmax <= 32
    OR snow >= 1;
"""

# Query database and assign result to wintry_days
wintry_days = pd.read_sql(query, engine)

# View summary stats about the temperatures
print(wintry_days.describe())

// Getting distinct values
# Create query for unique combinations of borough and complaint_type
query = """
SELECT DISTINCT borough, 
       complaint_type
  FROM hpd311calls;
"""

# Load results of query to a dataframe
issues_and_boros = pd.read_sql(query, engine)

# Check assumption about issues and boroughs
print(issues_and_boros)
=> Output: <script.py> output:
              borough    complaint_type
    0           BRONX    HEAT/HOT WATER
    1       MANHATTAN          PLUMBING
    2       MANHATTAN    HEAT/HOT WATER
    3        BROOKLYN    HEAT/HOT WATER
    4          QUEENS    HEAT/HOT WATER
    ..            ...               ...
    60      MANHATTAN  OUTSIDE BUILDING
    61      MANHATTAN          ELEVATOR
    62       BROOKLYN  OUTSIDE BUILDING
    63  STATEN ISLAND            SAFETY
    64  STATEN ISLAND  OUTSIDE BUILDING
    
    [65 rows x 2 columns]

// Counting in groups
# Create query to get call counts by complaint_type
query = """
SELECT complaint_type, 
       COUNT(*)
  FROM hpd311calls
 GROUP BY complaint_type;
"""

# Create dataframe of call counts by issue
calls_by_issue = pd.read_sql(query, engine)

# Graph the number of calls for each housing issue
calls_by_issue.plot.barh(x="complaint_type")
plt.show()

// Working with aggregate functions
# Create query to get temperature and precipitation by month
query = """
SELECT month, 
       MAX(tmax), 
       MIN(tmin),
       SUM(prcp)
  FROM weather 
 GROUP BY month;
"""

# Get dataframe of monthly weather stats
weather_by_month = pd.read_sql(query, engine)

# View weather stats by month
print(weather_by_month)
=> Output: <script.py> output:
          month  MAX(tmax)  MIN(tmin)  SUM(prcp)
    0  December         61          9       2.21
    1  February         78         16       5.83
    2   January         61          5       2.18
    3     March         62         27       5.17

// Joining tables
# Query to join weather to call records by date columns
query = """
SELECT * 
  FROM hpd311calls
  JOIN weather 
  ON hpd311calls.created_date = weather.date;
"""

# Create dataframe of joined tables
calls_with_weather = pd.read_sql(query, engine)

# View the dataframe to make sure all columns were joined
print(calls_with_weather.head())

// Joining and filtering
# Query to get water leak calls and daily precipitation
query = """
SELECT hpd311calls.*, weather.prcp
  FROM hpd311calls
  JOIN weather
    ON hpd311calls.created_date = weather.date
 WHERE hpd311calls.complaint_type = 'WATER LEAK';"""

# Load query results into the leak_calls dataframe
leak_calls = pd.read_sql(query, engine)

# View the dataframe
print(leak_calls.head())

// Joining, filtering, and aggregating
# Modify query to join tmax and tmin from weather by date
query = """
SELECT hpd311calls.created_date, 
       COUNT(*), 
       weather.tmax, 
       weather.tmin
  FROM hpd311calls 
       JOIN weather 
       ON hpd311calls.created_date = weather.date
 WHERE hpd311calls.complaint_type = 'HEAT/HOT WATER' 
 GROUP BY hpd311calls.created_date;
 """

# Query database and save results as df
df = pd.read_sql(query, engine)

# View first 5 records
print(df.head())
=> Output: <script.py> output:
      created_date  COUNT(*)  tmax  tmin
    0   01/01/2018      4597    19     7
    1   01/02/2018      4362    26    13
    2   01/03/2018      3045    30    16
    3   01/04/2018      3374    29    19
    4   01/05/2018      4333    19     9

// Load JSON data
# Load pandas as pd
import pandas as pd

# Load the daily report to a dataframe
pop_in_shelters = pd.read_json("dhs_daily_report.json")

# View summary stats about pop_in_shelters
print(pop_in_shelters.describe())

// Work with JSON orientations
try:
    # Load the JSON with orient specified
    df = pd.read_json("dhs_report_reformatted.json",
                      orient="split")
    
    # Plot total population in shelters over time
    df["date_of_census"] = pd.to_datetime(df["date_of_census"])
    df.plot(x="date_of_census", 
            y="total_individuals_in_shelter")
    plt.show()
    
except ValueError:
    print("pandas could not parse the JSON.")

// Get data from an API
api_url = "https://api.yelp.com/v3/businesses/search"

# Get data about NYC cafes from the Yelp API
response = requests.get(api_url, 
                        headers=headers, 
                        params=params)

# Extract JSON data from the response
data = response.json()

# Load data to a dataframe
cafes = pd.DataFrame(data["businesses"])

# View the data's dtypes
print(cafes.dtypes)
=> Output: <script.py> output:
    id                object
    alias             object
    name              object
    image_url         object
    is_closed           bool
    url               object
    review_count       int64
    categories        object
    rating           float64
    coordinates       object
    transactions      object
    location          object
    phone             object
    display_phone     object
    distance         float64
    price             object
    dtype: object

// Set API parameters
# Create dictionary to query API for cafes in NYC
parameters = {"term": "cafe",
          	  "location": "NYC"}

# Query the Yelp API with headers and params set
response = requests.get(api_url, 
                        headers=headers, 
                        params=parameters)

# Extract JSON data from response
data = response.json()

# Load "businesses" values to a dataframe and print head
cafes = pd.DataFrame(data["businesses"])
print(cafes.head())
=> Output: <script.py> output:
                           id                        alias               name                                          image_url  is_closed  ...                                           location  \
    0  CBmrwh7jHn88M4v8Q9Qyyg       white-noise-brooklyn-2        White Noise  https://s3-media2.fl.yelpcdn.com/bphoto/rcNRZr...      False  ...  {'address1': '71 Smith St', 'address2': '', 'a...   
    1  NG-KlDcSjBk3pfdzjXmMVw          devocion-brooklyn-3           Devocion  https://s3-media1.fl.yelpcdn.com/bphoto/xEKPpn...      False  ...  {'address1': '276 Livingston St', 'address2': ...   
    2  pimuUR-TEHIjUla3S3jemQ   coffee-project-ny-new-york  Coffee Project NY  https://s3-media2.fl.yelpcdn.com/bphoto/2Wtg4i...      False  ...  {'address1': '239 E 5th St', 'address2': None,...   
    3  wTA00Dov2lFYXKnZ58eXAQ  spreadhouse-cafe-new-york-3   Spreadhouse Cafe  https://s3-media2.fl.yelpcdn.com/bphoto/YsBfwR...      False  ...  {'address1': '116 Suffolk St', 'address2': '',...   
    4  vijwGDNrPBJHEG7_DsjZNw             usagi-ny-dumbo-7           Usagi NY  https://s3-media2.fl.yelpcdn.com/bphoto/5gCxDD...      False  ...  {'address1': '163 Plymouth St', 'address2': ''...   
    
              phone   display_phone  distance price  
    0                                1856.127   NaN  
    1  +17182856180  (718) 285-6180  2087.817    $$  
    2  +12122287888  (212) 228-7888  2435.843    $$  
    3  +16465246353  (646) 524-6353  1657.233     $  
    4  +17188018037  (718) 801-8037   635.782    $$  
    
    [5 rows x 16 columns]

// Set request headers
# Create dictionary that passes Authorization and key string
headers = {"Authorization": "Bearer {}".format(api_key)}

# Query the Yelp API with headers and params set
response = requests.get(api_url, 
                        headers=headers, 
                        params=params)

# Extract JSON data from response
data = response.json()

# Load "businesses" values to a dataframe and print names
cafes = pd.DataFrame(data["businesses"])
print(cafes.name)
=> Output: <script.py> output:
    0             Coffee Project NY
    1                Urban Backyard
    2              Saltwater Coffee
    3                 Bird & Branch
    4                  Bibble & Sip
    5             Coffee Project NY
    6                        Burrow
    7                   Cafe Patoro
    8                     Sweatshop
    9                       Round K
    10               Kobrick Coffee
    11            Kaigo Coffee Room
    12              Absolute Coffee
    13                     Devocion
    14                The Uncommons
    15                      Butler 
    16              Cafe Hanamizuki
    17    Brooklyn Roasting Company
    18             Takahachi Bakery
    19              Happy Bones NYC
    Name: name, dtype: object

// Flatten nested JSONs
# Load json_normalize()
from pandas.io.json import json_normalize

# Isolate the JSON data from the API response
data = response.json()

# Flatten business data into a dataframe, replace separator
cafes = json_normalize(data["businesses"],
                       sep="_")

# View data
print(cafes.head())
=> Output: <script.py> output:
                           id                        alias               name                                          image_url  is_closed  ... location_zip_code  location_country location_state  \
    0  CBmrwh7jHn88M4v8Q9Qyyg       white-noise-brooklyn-2        White Noise  https://s3-media2.fl.yelpcdn.com/bphoto/rcNRZr...      False  ...             11201                US             NY   
    1  NG-KlDcSjBk3pfdzjXmMVw          devocion-brooklyn-3           Devocion  https://s3-media1.fl.yelpcdn.com/bphoto/xEKPpn...      False  ...             11201                US             NY   
    2  pimuUR-TEHIjUla3S3jemQ   coffee-project-ny-new-york  Coffee Project NY  https://s3-media2.fl.yelpcdn.com/bphoto/2Wtg4i...      False  ...             10003                US             NY   
    3  wTA00Dov2lFYXKnZ58eXAQ  spreadhouse-cafe-new-york-3   Spreadhouse Cafe  https://s3-media2.fl.yelpcdn.com/bphoto/YsBfwR...      False  ...             10002                US             NY   
    4  vijwGDNrPBJHEG7_DsjZNw             usagi-ny-dumbo-7           Usagi NY  https://s3-media2.fl.yelpcdn.com/bphoto/5gCxDD...      False  ...             11201                US             NY   
    
                      location_display_address price  
    0        [71 Smith St, Brooklyn, NY 11201]   NaN  
    1  [276 Livingston St, Brooklyn, NY 11201]    $$  
    2       [239 E 5th St, New York, NY 10003]    $$  
    3     [116 Suffolk St, New York, NY 10002]     $  
    4       [163 Plymouth St, Dumbo, NY 11201]    $$  
    
    [5 rows x 24 columns]

// Handle deeply nested data
# Flatten businesses records and set underscore separators
flat_cafes = json_normalize(data["businesses"],
                            sep="_")

# View the data
print(flat_cafes.head())
=> Output: <script.py> output:
                           id                        alias               name                                          image_url  is_closed  ... location_zip_code  location_country location_state  \
    0  CBmrwh7jHn88M4v8Q9Qyyg       white-noise-brooklyn-2        White Noise  https://s3-media2.fl.yelpcdn.com/bphoto/rcNRZr...      False  ...             11201                US             NY   
    1  NG-KlDcSjBk3pfdzjXmMVw          devocion-brooklyn-3           Devocion  https://s3-media1.fl.yelpcdn.com/bphoto/xEKPpn...      False  ...             11201                US             NY   
    2  pimuUR-TEHIjUla3S3jemQ   coffee-project-ny-new-york  Coffee Project NY  https://s3-media2.fl.yelpcdn.com/bphoto/2Wtg4i...      False  ...             10003                US             NY   
    3  wTA00Dov2lFYXKnZ58eXAQ  spreadhouse-cafe-new-york-3   Spreadhouse Cafe  https://s3-media2.fl.yelpcdn.com/bphoto/YsBfwR...      False  ...             10002                US             NY   
    4  vijwGDNrPBJHEG7_DsjZNw             usagi-ny-dumbo-7           Usagi NY  https://s3-media2.fl.yelpcdn.com/bphoto/5gCxDD...      False  ...             11201                US             NY   
    
                      location_display_address price  
    0        [71 Smith St, Brooklyn, NY 11201]   NaN  
    1  [276 Livingston St, Brooklyn, NY 11201]    $$  
    2       [239 E 5th St, New York, NY 10003]    $$  
    3     [116 Suffolk St, New York, NY 10002]     $  
    4       [163 Plymouth St, Dumbo, NY 11201]    $$  
    
    [5 rows x 24 columns]

# Specify record path to get categories data
flat_cafes = json_normalize(data["businesses"],
                            sep="_",
                    		record_path="categories")

# View the data
print(flat_cafes.head())
=> Output: <script.py> output:
                  alias              title
    0            coffee       Coffee & Tea
    1            coffee       Coffee & Tea
    2  coffeeroasteries  Coffee Roasteries
    3             cafes              Cafes
    4            coffee       Coffee & Tea

# Load other business attributes and set meta prefix
flat_cafes = json_normalize(data["businesses"],
                            sep="_",
                    		record_path="categories",
                    		meta=["name", 
                                  "alias",  
                                  "rating",
                          		  ["coordinates", "latitude"], 
                          		  ["coordinates", "longitude"]],
                    		meta_prefix="biz_")

# View the data
print(flat_cafes.head())
=> Output: <script.py> output:
                  alias              title           biz_name                   biz_alias biz_rating biz_coordinates_latitude biz_coordinates_longitude
    0            coffee       Coffee & Tea        White Noise      white-noise-brooklyn-2        4.5                   40.689                   -73.988
    1            coffee       Coffee & Tea           Devocion         devocion-brooklyn-3        4.0                   40.689                   -73.983
    2  coffeeroasteries  Coffee Roasteries           Devocion         devocion-brooklyn-3        4.0                   40.689                   -73.983
    3             cafes              Cafes           Devocion         devocion-brooklyn-3        4.0                   40.689                   -73.983
    4            coffee       Coffee & Tea  Coffee Project NY  coffee-project-ny-new-york        4.5                   40.727                   -73.989

// Concatenate dataframes
# Add an offset parameter to get cafes 51-100
params = {"term": "cafe", 
          "location": "NYC",
          "sort_by": "rating", 
          "limit": 50,
          "offset": 50}

result = requests.get(api_url, headers=headers, params=params)
next_50_cafes = json_normalize(result.json()["businesses"])
# Concatenate the results, setting ignore_index to renumber rows
cafes = pd.concat([top_50_cafes, next_50_cafes], ignore_index=True)

# Print shape of cafes
print(cafes.shape)

// Merge dataframes
# Merge crosswalk into cafes on their zip code fields
cafes_with_pumas = cafes.merge(crosswalk, 
                   			   left_on="location_zip_code", 
                               right_on="zipcode")

# Merge pop_data into cafes_with_pumas on puma field
cafes_with_pop = cafes_with_pumas.merge(pop_data, on="puma")

# View the data
print(cafes_with_pop.head())
=> Output: <script.py> output:
                            alias                                         categories  coordinates_latitude  coordinates_longitude   display_phone  ...  geo_type  \
    0  coffee-project-ny-new-york     [{'alias': 'coffee', 'title': 'Coffee & Tea'}]                40.727                -73.989  (212) 228-7888  ...  PUMA2010   
    1   saltwater-coffee-new-york     [{'alias': 'coffee', 'title': 'Coffee & Tea'}]                40.730                -73.984  (917) 881-2245  ...  PUMA2010   
    2   daily-provisions-new-york  [{'alias': 'cafes', 'title': 'Cafes'}, {'alias...                40.738                -73.988  (212) 488-1505  ...  PUMA2010   
    3              mud-new-york-3  [{'alias': 'coffee', 'title': 'Coffee & Tea'},...                40.729                -73.987  (212) 228-9074  ...  PUMA2010   
    4  coffee-project-ny-new-york     [{'alias': 'coffee', 'title': 'Coffee & Tea'}]                40.727                -73.989  (212) 228-7888  ...  PUMA2010   
    
                                               geog_name    borough  total_pop_estimate total_pop_moe  
    0  NYC-Manhattan Community District 3--Chinatown ...  Manhattan              160709          3289  
    1  NYC-Manhattan Community District 3--Chinatown ...  Manhattan              160709          3289  
    2  NYC-Manhattan Community District 3--Chinatown ...  Manhattan              160709          3289  
    3  NYC-Manhattan Community District 3--Chinatown ...  Manhattan              160709          3289  
    4  NYC-Manhattan Community District 3--Chinatown ...  Manhattan              160709          3289  
    
    [5 rows x 37 columns]
