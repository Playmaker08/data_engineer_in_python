// Running an ETL Pipeline
Here, the functions extract(), transform(), and load() have been defined for you. To run this data ETL pipeline, you're going to execute each of these functions. If you're curious, take a peek at what the extract() function looks like.

def extract(file_name):
    print(f"Extracting data from {file_name}")
    return pd.read_csv(file_name)

# Extract data from the raw_data.csv file
extracted_data = extract(file_name="raw_data.csv")

# Transform the extracted_data
transformed_data = transform(data_frame=extracted_data)

# Load the transformed_data to cleaned_data.csv
load(data_frame=transformed_data, target_table="cleaned_data")
=> Output: <script.py> output:
    Extracting data from raw_data.csv.
    Transforming 96 rows of raw data.
    Loading cleaned data to cleaned_data.

// ELT in Action
# Extract data from the raw_data.csv file
raw_data = extract(file_name="raw_data.csv")

# Load the extracted_data to the raw_data table
load(data_frame=raw_data, table_name="raw_data")

# Transform data in the raw_data table
transform(
  source_table="raw_data", 
  target_table="cleaned_data"
)
=> Output: <script.py> output:
    Extracting data from raw_data.csv.
    Loading cleaned data to raw_data.
    Transformed data with the query: 
      	CREATE TABLE cleaned_data AS
          SELECT
              CONCAT("Product ID: ", product_id),
              quantity * price
          FROM raw_data;

// Building an ETL Pipeline
def load(data_frame, file_name):
  # Write cleaned_data to a CSV using file_name
  data_frame.to_csv(file_name)
  print(f"Successfully loaded data to {file_name}")

extracted_data = extract(file_name="raw_data.csv")

# Transform extracted_data using transform() function
transformed_data = transform(data_frame=extracted_data)

# Load transformed_data to the file transformed_data.csv
load(data_frame=transformed_data, file_name="transformed_data.csv")

// The "T" in ELT
# Complete building the transform() function
def transform(source_table, target_table):
  data_warehouse.execute(f"""
  CREATE TABLE {target_table} AS
      SELECT
          CONCAT("Product ID: ", product_id),
          quantity * price
      FROM {source_table};
  """)

extracted_data = extract(file_name="raw_sales_data.csv")
load(data_frame=extracted_data, table_name="raw_sales_data")

# Populate total_sales by transforming raw_sales_data
transform(source_table="raw_sales_data", target_table="total_sales")
=> <script.py> output:
    Extracted data from file storage.
    Loading extracted data to sale_items.
    Ran the query: 
      CREATE TABLE total_sales AS
          SELECT
              CONCAT("Product ID: ", product_id),
              quantity * price
          FROM raw_sales_data;

// Extracting, Transforming, and Loading Student Scores Data
def extract(file_name):
  # Read a CSV with a path stored using file_name into memory
  return pd.read_csv(file_name)

def transform(data_frame):
  # Filter the data_frame to only incude a subset of columns
  return data_frame.loc[:, ["industry_name", "number_of_firms"]]

def load(data_frame, file_name):
  # Write the data_frame to a CSV
  data_frame.to_csv(file_name)

extracted_data = extract(file_name="raw_industry_data.csv")
transformed_data = transform(data_frame=extracted_data)

# Pass the transformed_data DataFrame to the load() function
load(data_frame=transformed_data, file_name="number_of_firms.csv")

// Extracting data from parquet files
import pandas as pd

# Read the sales data into a DataFrame
sales_data = pd.read_parquet("sales_data.parquet", engine="fastparquet")

# Check the data type of the columns of the DataFrames
print(sales_data.dtypes)

# Print the shape of the DataFrame, as well as the head
print(sales_data.shape)
print(sales_data.head())
=> Output: <script.py> output:
    Order ID              int64
    Product              object
    Quantity Ordered      int64
    Price Each          float64
    Order Date           object
    Purchase Address     object
    dtype: object
    (282, 6)
       Order ID                 Product  Quantity Ordered  Price Each      Order Date                           Purchase Address
    0    259358  34in Ultrawide Monitor                 1      379.99  10/28/19 10:56            609 Cherry St, Dallas, TX 75001
    1    259359  27in 4K Gaming Monitor                 1      389.99  10/28/19 17:26          225 5th St, Los Angeles, CA 90001
    2    259360  AAA Batteries (4-pack)                 2        2.99  10/24/19 17:20       967 12th St, New York City, NY 10001
    3    259361        27in FHD Monitor                 1      149.99  10/14/19 22:26  628 Jefferson St, New York City, NY 10001
    4    259362        Wired Headphones                 1       11.99   10/7/19 16:10         534 14th St, Los Angeles, CA 90001

// Pulling data from SQL databases
import sqlalchemy

# Create a connection to the sales database
db_engine = sqlalchemy.create_engine("postgresql+psycopg2://repl:password@localhost:5432/sales")

# Query all rows and columns of the sales table
raw_sales_data = pd.read_sql("SELECT * FROM sales", db_engine)
print(raw_sales_data)
=> Output: <script.py> output:
         order_id                   product  quantity_ordered  price_each      order_date                           purchase_address
    0      259358    34in Ultrawide Monitor                 1      379.99  10/28/19 10:56            609 Cherry St, Dallas, TX 75001
    1      259359    27in 4K Gaming Monitor                 1      389.99  10/28/19 17:26          225 5th St, Los Angeles, CA 90001
    2      259360    AAA Batteries (4-pack)                 2        2.99  10/24/19 17:20       967 12th St, New York City, NY 10001
    3      259361          27in FHD Monitor                 1      149.99  10/14/19 22:26  628 Jefferson St, New York City, NY 10001
    4      259362          Wired Headphones                 1       11.99   10/7/19 16:10         534 14th St, Los Angeles, CA 90001
    ..        ...                       ...               ...         ...             ...                                        ...
    277    259623  Apple Airpods Headphones                 1      150.00  10/20/19 21:12      128 River St, San Francisco, CA 94016
    278    259624    34in Ultrawide Monitor                 1      379.99  10/30/19 12:45                60 9th St, Austin, TX 73301
    279    259625    AAA Batteries (4-pack)                 1        2.99  10/30/19 18:37       946 13th St, San Francisco, CA 94016
    280    259626    AAA Batteries (4-pack)                 2        2.99  10/22/19 15:15            419 South St, Seattle, WA 98101
    281    259627    AAA Batteries (4-pack)                 1        2.99   10/9/19 20:05        30 Lake St, San Francisco, CA 94016
    
    [282 rows x 6 columns]

// Building functions to extract data
def extract():
  	# Create a connection URI and connection engine
    connection_uri = "postgresql+psycopg2://repl:password@localhost:5432/sales"
    db_engine = sqlalchemy.create_engine(connection_uri)

    # Query the DataFrame to return all records with quantity_ordered equal to 1
    raw_sales_data = pd.read_sql("SELECT * FROM sales WHERE quantity_ordered = 1", db_engine)

    # Print the head of the DataFrame
    print(raw_data.head())
    
    # Return the extracted DataFrame
    return raw_data
    
# Call the extract() function
raw_sales_data = extract()
=> Output: <script.py> output:
       order_id                 product  quantity_ordered  price_each      order_date                           purchase_address
    0    259358  34in Ultrawide Monitor                 1      379.99  10/28/19 10:56            609 Cherry St, Dallas, TX 75001
    1    259359  27in 4K Gaming Monitor                 1      389.99  10/28/19 17:26          225 5th St, Los Angeles, CA 90001
    2    259361        27in FHD Monitor                 1      149.99  10/14/19 22:26  628 Jefferson St, New York City, NY 10001
    3    259362        Wired Headphones                 1       11.99   10/7/19 16:10         534 14th St, Los Angeles, CA 90001
    4    259363  AAA Batteries (4-pack)                 1        2.99   10/1/19 18:55       976 Lake St, New York City, NY 10001

// Filtering pandas DataFrames
# Extract data from the sales_data.parquet path
raw_sales_data = extract("sales_data.parquet")

def transform(raw_data):
  	# Only keep rows with `Quantity Ordered` greater than 1
    clean_data = raw_data.loc[raw_data["Quantity Ordered"] > 1, :]
	
    # Only keep columns "Order Date", "Quantity Ordered", and "Purchase Address"
    clean_data = clean_data.loc[:, ["Order Date", "Quantity Ordered", "Purchase Address"]]
	
    # Return the filtered DataFrame
    return clean_data
    
transform(raw_sales_data)

// Transforming sales data with pandas
raw_sales_data = extract("sales_data.csv")

def transform(raw_data):
    # Convert the "Order Date" column to type datetime
    raw_data["Order Date"] = pd.to_datetime(raw_data["Order Date"], format="%m/%d/%y %H:%M")
    
    # Only keep items under ten dollars
    clean_data = raw_data.loc[raw_data["Price Each"] < 10, :]
    return clean_data

clean_sales_data = transform(raw_sales_data)

# Check the data types of each column
print(clean_sales_data.dtypes)
=> Output: <script.py> output:
    Order ID                     int64
    Product                     object
    Quantity Ordered             int64
    Price Each                 float64
    Order Date          datetime64[ns]
    Purchase Address            object
    dtype: object

// Validating data transformations
def extract(file_path):
  	# Ingest the data to a DataFrame
    raw_data = pd.read_parquet(file_path)
    
    # Return the DataFrame
    return raw_data
  
raw_sales_data = extract("sales_data.parquet")

def transform(raw_data):
  	# Filter rows and columns
    clean_data = raw_data.loc[raw_data["Quantity Ordered"] == 1, ["Order ID", "Price Each", "Quantity Ordered"]]
    return clean_data

# Transform the raw_sales_data
clean_sales_data = transform(raw_sales_data)

// Loading sales data to a CSV file
def transform(raw_data):
	# Find the items prices less than 25 dollars
	return raw_data.loc[raw_data["Price Each"] < 25, ["Order ID", "Product", "Price Each", "Order Date"]]

def load(clean_data):
	# Write the data to a CSV file without the index column
	clean_data.to_csv("transformed_sales_data.csv", index=False)


clean_sales_data = transform(raw_sales_data)

# Call the load function on the cleaned DataFrame
load(clean_sales_data)

// Customizing a CSV file
# Import the os library
import os

# Load the data to a csv file with the index, no header and pipe separated
def load(clean_data, path_to_write):
	clean_data.to_csv(path_to_write, header=False, sep="|")

load(clean_sales_data, "clean_sales_data.csv")

# Check that the file is present.
file_exists = os.path.exists("clean_sales_data.csv")
print(file_exists)

// Persisting data to files
def load(clean_data, file_path):
    # Write the data to a file
    clean_data.to_csv(file_path, header=False, index=False)

    # Check to make sure the file exists
    file_exists = os.path.exists(file_path)
    if not file_exists:
        raise Exception(f"File does NOT exists at path {file_path}")

# Load the transformed data to the provided file path
load(clean_sales_data, "transformed_sales_data.csv")

// Logging within a data pipeline
def transform(raw_data):
    raw_data["Order Date"] = pd.to_datetime(raw_data["Order Date"], format="%m/%d/%y %H:%M")
    clean_data = raw_data.loc[raw_data["Price Each"] < 10, :]
    
    # Create an info log regarding transformation
    logging.info("Transformed 'Order Date' column to type 'datetime'.")
    
    # Create debug-level logs for the DataFrame before and after filtering
    logging.debug(f"Shape of the DataFrame before filtering: {raw_data.shape}")
    logging.debug(f"Shape of the DataFrame after filtering: {clean_data.shape}")
    
    return clean_data
  
clean_sales_data = transform(raw_sales_data)

// Handling exceptions when loading data
def extract(file_path):
    return pd.read_parquet(file_path)

# Update the pipeline to include a try block
try:
	# Attempt to read in the file
    raw_sales_data = extract("sales_data.parquet")
	
# Catch the FileNotFoundError
except FileNotFoundError as file_not_found:
	# Write an error-level log
	logging.error(file_not_found)

// Monitoring and alerting within a data pipeline
def transform(raw_data):
	return raw_data.loc[raw_data["Total Price"] > 1000, :]

try:
	# Attempt to transform DataFrame, log an info-level message
	clean_sales_data = transform(raw_sales_data)
	logging.info("Successfully filtered DataFrame by 'Total Price'")
	
# Update the exception to be a KeyError, alias as ke
except KeyError as ke:
	# Log a warning-level message
	logging.warning(f"{ke}: Cannot filter DataFrame by 'Total Price'")
 
 # Create the "Total Price" column, transform the updated DataFrame
	raw_sales_data["Total Price"] = raw_sales_data["Price Each"] * raw_sales_data["Quantity Ordered"]
	clean_sales_data = transform(raw_sales_data)

// Ingesting JSON data with pandas
def extract(file_path):
  # Read the JSON file into a DataFrame
  return pd.read_json(file_path, orient="records")

# Call the extract function with the appropriate path, assign to raw_testing_scores
raw_testing_scores = extract("testing_scores.json")

# Output the head of the DataFrame
print(raw_testing_scores.head())
=> Output: <script.py> output:
                  street_address       city  math_score  reading_score  writing_score
    02M260  425 West 33rd Street  Manhattan         NaN            NaN            NaN
    06M211    650 Academy Street  Manhattan         NaN            NaN            NaN
    01M539   111 Columbia Street  Manhattan       657.0          601.0          601.0
    02M294      350 Grand Street  Manhattan       395.0          411.0          387.0
    02M308      350 Grand Street  Manhattan       418.0          428.0          415.0

// Reading JSON data into memory
1. Use pandas to read a JSON file into a DataFrame.
def extract(file_path):
  	# Read the JSON file into a DataFrame, orient by index
	return pd.read_json(file_path, orient="index")

# Call the extract function, pass in the desired file_path
raw_testing_scores = extract("nested_scores.json")
print(raw_testing_scores.head())

2. Import the json library. Use the json library to load the "nested_scores.json" file into memory.
# Import the json library
import json

def extract(file_path):
    with open(file_path, "r") as json_file:
        # Load the data from the JSON file
        raw_data = json.load(json_file)
    return raw_data

raw_testing_scores = extract("nested_scores.json")

# Print the raw_testing_scores
print(raw_testing_scores)

// Iterating over dictionaries
The "nested_school_scores.json" file has been read into a dictionary stored in the raw_testing_scores variable, which takes the following form:
{
    "01M539": {
        "street_address": "111 Columbia Street",
        "city": "Manhattan",
        "scores": {
              "math": 657,
              "reading": 601,
              "writing": 601
        }
  }, ...
}

1. Loop through the keys of the raw_testing_scores dictionary. Add each key to the raw_testing_scores_keys list.
raw_testing_scores_keys = []

# Iterate through the keys of the raw_testing_scores dictionary
for school_id in raw_testing_scores.keys():
  	# Append each key to the raw_testing_scores_keys list
	raw_testing_scores_keys.append(school_id)
    
print(raw_testing_scores_keys[0:3])

2. Now, loop through a list of values from the raw_testing_scores dictionary.
raw_testing_scores_values = []

# Iterate through the values of the raw_testing_scores dictionary
for school_info in raw_testing_scores.values():
	raw_testing_scores_values.append(school_info)
    
print(raw_testing_scores_values[0:3])

3. Finally, loop through both the keys and values of the raw_testing_scores dictionary, simultaneously.
raw_testing_scores_keys = []
raw_testing_scores_values = []

# Iterate through the values of the raw_testing_scores dictionary
for school_id, school_info in raw_testing_scores.items():
	raw_testing_scores_keys.append(school_id)
	raw_testing_scores_values.append(school_info)

print(raw_testing_scores_keys[0:3])
print(raw_testing_scores_values[0:3])

// Parsing data from dictionaries
The dictionary below is stored in the school variable. Good luck!

{
    "street_address": "111 Columbia Street",
    "city": "Manhattan",
    "scores": {
        "math": 657,
        "reading": 601
    }
}

# Parse the street_address from the dictionary
street_address = school.get("street_address")

# Parse the scores dictionary
scores = school.get("scores")

# Try to parse the math, reading and writing values from scores
math_score = scores.get("math", 0)
reading_score = scores.get("reading", 0)
writing_score = scores.get("writing", 0)

print(f"Street Address: {street_address}")
print(f"Math: {math_score}, Reading: {reading_score}, Writing: {writing_score}")
=> Output: <script.py> output:
    Street Address: 111 Columbia Street
    Math: 657, Reading: 601, Writing: 0

// Transforming JSON data
The "nested_school_scores.json" file has been read into a dictionary available in the raw_testing_scores variable, which takes the following form:
{
    "01M539": {
        "street_address": "111 Columbia Street",
        "city": "Manhattan",
        "scores": {
              "math": 657,
              "reading": 601,
              "writing": 601
        }
  }, ...
}

normalized_testing_scores = []

# Loop through each of the dictionary key-value pairs
for school_id, school_info in raw_testing_scores.items():
	normalized_testing_scores.append([
    	school_id,
    	school_info.get("street_address"),  # Pull the "street_address"
    	school_info.get("city"),
    	school_info.get("scores").get("math", 0),
    	school_info.get("scores").get("reading", 0),
    	school_info.get("scores").get("writing", 0),
    ])

print(normalized_testing_scores)

// Transforming and cleaning DataFrames
Per usual, pandas has been imported as pd, and the normalized_testing_scores variable stores the list of each schools testing data, as shown below.

[
    ['01M539', '111 Columbia Street', 'Manhattan', 657.0, 601.0, 601.0],
    ...
]   

# Create a DataFrame from the normalized_testing_scores list
normalized_data = pd.DataFrame(normalized_testing_scores)

# Set the column names
normalized_data.columns = ["school_id", "street_address", "city", "avg_score_math", "avg_score_reading", "avg_score_writing"]

normalized_data = normalized_data.set_index("school_id")
print(normalized_data.head())

// Filling missing values with pandas
# Print the head of the `raw_testing_scores` DataFrame
print(raw_testing_scores.head())

# Fill NaN values with the average from that column
raw_testing_scores["math_score"] = raw_testing_scores["math_score"].fillna(raw_testing_scores["math_score"].mean())

# Print the head of the raw_testing_scores DataFrame
print(raw_testing_scores.head())

def transform(raw_data):
	raw_data.fillna(
    	value={
			# Fill NaN values with column mean
			"math_score": raw_data["math_score"].mean(),
			"reading_score": raw_data["reading_score"].mean(),
			"writing_score": raw_data["writing_score"].mean()
		}, inplace=True
	)
	return raw_data

clean_testing_scores = transform(raw_testing_scores)

# Print the head of the clean_testing_scores DataFrame
print(clean_testing_scores.head())

// Grouping data with pandas
def transform(raw_data):
	# Use .loc[] to only return the needed columns
	raw_data = raw_data.loc[:, ["city", "math_score", "reading_score", "writing_score"]]
	
    # Group the data by city, return the grouped DataFrame
	grouped_data = raw_data.groupby(by=["city"], axis=0).mean()
	return grouped_data

# Transform the data, print the head of the DataFrame
grouped_testing_scores = transform(raw_testing_scores)
print(grouped_testing_scores.head())

// Applying advanced transformations to DataFrames
def transform(raw_data):
	# Use the apply function to extract the street_name from the street_address
    raw_data["street_name"] = raw_data.apply(
    	# Pass the correct function to the apply method
        find_street_name,
        axis=1
    )
    return raw_data

# Transform the raw_testing_scores DataFrame
cleaned_testing_scores = transform(raw_testing_scores)

# Print the head of the cleaned_testing_scores DataFrame
print(cleaned_testing_scores.head())

// Loading data to a Postgres database
sqlalchemy has been imported, and pandas is available as pd. The first few rows of the cleaned_testing_scores DataFrame are shown below:

             street_address       city  math_score  ... best_score
01M539  111 Columbia Street  Manhattan       657.0      Math
02M545     350 Grand Street  Manhattan       613.0      Math
01M292     220 Henry Street  Manhattan       410.0      Math

# Update the connection string, create the connection object to the schools database
db_engine = sqlalchemy.create_engine("postgresql+psycopg2://repl:password@localhost:5432/schools")

# Write the DataFrame to the scores table
cleaned_testing_scores.to_sql(
	name="scores",
	con=db_engine,
	index=False,
	if_exists="replace"
)

// Validating data loaded to a Postgres Database
def load(clean_data, con_engine):
    # Store the data in the schools database
    clean_data.to_sql(name="scores_by_city", con=con_engine, if_exists="replace", index=True, index_label="school_id")
    
# Call the load function, passing in the cleaned DataFrame
load(cleaned_testing_scores, db_engine)

# Call query the data in the scores_by_city table, check the head of the DataFrame
to_validate = pd.read_sql("SELECT * FROM scores_by_city", con=db_engine)
print(to_validate.head())

// Validating a data pipeline at "checkpoints"
# Extract and transform tax_data
raw_tax_data = extract("raw_tax_data.csv")
clean_tax_data = transform(raw_tax_data)
load(clean_tax_data, "clean_tax_data.parquet")

# Check the shape of the raw_tax_data DataFrame, compare to the clean_tax_data DataFrame
print(f"Shape of raw_tax_data: {raw_tax_data.shape}")
print(f"Shape of clean_tax_data: {clean_tax_data.shape}")

# Read in the loaded data, observe the head of each
to_validate = pd.read_parquet("clean_tax_data.parquet")
print(clean_tax_data.head(3))
print(to_validate.head(3))

# Check that the DataFrames are equal
print(to_validate.equals(clean_tax_data))
=> Output: <script.py> output:
    Shape of raw_tax_data: (96, 6)
    Shape of clean_tax_data: (82, 5)

                       number_of_firms  total_taxable_income  total_taxes_paid  total_cash_taxes_paid  average_taxable_income
    industry_name                                                                                                            
    Aerospace/Defense               77             30920.169          5106.376               7441.776                 401.561
    Apparel                         39              5422.690          1112.113               1479.292                 139.043
    Auto & Truck                    31             33358.200          3529.000               2446.896                1076.071
                       number_of_firms  total_taxable_income  total_taxes_paid  total_cash_taxes_paid  average_taxable_income
    industry_name                                                                                                            
    Aerospace/Defense               77             30920.169          5106.376               7441.776                 401.561
    Apparel                         39              5422.690          1112.113               1479.292                 139.043
    Auto & Truck                    31             33358.200          3529.000               2446.896                1076.071

    True

// Testing a data pipeline end-to-end
# Trigger the data pipeline to run three times
for attempt in range(0, 3):
	print(f"Attempt: {attempt}")
	raw_tax_data = extract("raw_tax_data.csv")
	clean_tax_data = transform(raw_tax_data)
	load(clean_tax_data, "clean_tax_data.parquet")
	
	# Print the shape of the cleaned_tax_data DataFrame
	print(f"Shape of clean_tax_data: {clean_tax_data.shape}")
    
# Read in the loaded data, check the shape
to_validate = pd.read_parquet("clean_tax_data.parquet")
print(f"Final shape of cleaned data: {to_validate.shape}")
=> Output: <script.py> output:
    Attempt: 0
    Shape of clean_tax_data: (82, 5)
    Attempt: 1
    Shape of clean_tax_data: (82, 5)
    Attempt: 2
    Shape of clean_tax_data: (82, 5)
    Final shape of cleaned data: (82, 5)
By testing this pipeline end-to-end, you've validated that the pipeline can be run multiple times, with data being made available to downstream consumers without duplication.

// Validating a data pipeline with assert
raw_tax_data = extract("raw_tax_data.csv")
clean_tax_data = transform(raw_tax_data)

# Validate the number of columns in the DataFrame
assert len(clean_tax_data.columns) == 5

# Determine if the clean_tax_data DataFrames take type pd.DataFrame
isinstance(clean_tax_data, pd.DataFrame)

# Assert that clean_tax_data is an instance of a pd.DataFrame
assert isinstance(clean_tax_data, pd.DataFrame)

# Assert that clean_tax_data takes is an instance of a string
try:
	assert isinstance(clean_tax_data, str)
except Exception as e:
	print(e)

// Writing unit tests with pytest
The functions extract() and transform() have been made available for you, along with pandas, which has been imported as pd. You'll be testing the transform() function, which is shown below.

def transform(raw_data):
    raw_data["average_taxable_income"] = raw_data["total_taxable_income"] / raw_data["number_of_firms"]
    clean_data = raw_data.loc[raw_data["average_taxable_income"] > 100, :]
    clean_data.set_index("industry_name", inplace=True)
    return clean_data

import pytest
def test_transformed_data():
    raw_tax_data = extract("raw_tax_data.csv")
    clean_tax_data = transform(raw_tax_data)
    
    # Assert that the transform function returns a pd.DataFrame
    assert isinstance(clean_tax_data, pd.DataFrame)
    
    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame
    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)

// Creating fixtures with pytest
# Import pytest
import pytest

# Create a pytest fixture
@pytest.fixture()
def raw_tax_data():
	raw_data = extract("raw_tax_data.csv")
   
    # Return the raw DataFrame
	return raw_data

// Unit testing a data pipeline with fixtures
The transform function that you'll be building unit tests around is shown below for reference. pandas has been imported as pd, and the pytest() library is loaded and ready for use.

def transform(raw_data):
    raw_data["tax_rate"] = raw_data["total_taxes_paid"] / raw_data["total_taxable_income"]
    raw_data.set_index("industry_name", inplace=True)
    return raw_data

# Define a pytest fixture
@pytest.fixture()
def clean_tax_data():
    raw_data = pd.read_csv("raw_tax_data.csv")
    
    # Transform the raw_data, store in clean_data DataFrame, and return the variable
    clean_data = transform(raw_data)
    return clean_data

# Pass the fixture to the function
def test_tax_rate(clean_tax_data):
    # Assert values are within the expected range
    assert clean_tax_data["tax_rate"].max() <= 1 and clean_tax_data["tax_rate"].min() >= 0

What is the most popular orchestration tool for building, deploying, and monitoring data pipelines? Airflow

// Data pipeline architecture patterns
The project takes the following format, where pipeline_utils stores the extract(), transform(), and load() functions that will be used run the pipeline.

> ls
 etl_pipeline.py
 pipeline_utils.py

# Import the extract, transform, and load functions from pipeline_utils
from pipeline_utils import extract, transform, load

# Run the pipeline end to end by extracting, transforming and loading the data
raw_tax_data = extract("raw_tax_data.csv")
clean_tax_data = transform(raw_tax_data)
load(clean_tax_data, "clean_tax_data.parquet")

// Running a data pipeline end-to-end
import logging

# Import extract, transform, and load functions from pipeline_utils
from pipeline_utils import extract, transform, load

logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)

try:
  	# Extract, transform, and load the tax data
	raw_tax_data = extract("raw_tax_data.csv")
	clean_tax_data = transform(raw_tax_data)
	load(clean_tax_data, "clean_tax_data.parquet")
    
	logging.info("Successfully extracted, transformed and loaded data.")  # Log a success message.
    
except Exception as e:
	logging.error(f"Pipeline failed with error: {e}")  # Log failure message
